---
layout:     post
title:      Markov decision processes, reinforcement learning course outline
date:       2019-11-26 9:00:00
author:     Nathana&euml;l Fijalkow
category:   Reinforcement learning
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      Q: "{\\mathbb{Q}}",
      N: "{\\mathbb{N}}",
      Z: "{\\mathbb{Z}}",
      M: "{\\mathcal{M}}",
      A: "{\\mathcal{A}}",
      B: "{\\mathcal{B}}",
      E: "{\\mathbb{E}}",
      P: "{\\mathbb{P}}",
      val: "{\\text{val}}",
      Dist: "{\\mathcal{D}}",
    }
  }
});
</script>

<p class="intro"><span class="dropcap">W</span>e present the model of Markov decision processes and introduce the outline of a course on reinforcement learning.</p>

The reference book and key inspiration for this course and for reinforcement learning in general is the book by [Sutton and Barto](http://incompleteideas.net/book/the-book.html) (it can be downloaded for free at that address).
The reason for these lecture notes to exist is that we will have a more theoretical focus and we will provide toy code to play with the different algorithms.

The previous two blog posts on [multi-armed bandits]({{ '/blog/armed_bandits_upper_bound' | prepend: site.baseurl }}) and [the Monte Carlo tree search algorithm]({{ '/blog/armed_bandits_upper_bound' | prepend: site.baseurl }}) serve as introduction to the topic. The rest of the lecture will use Markov decision processes (MDP) as a model.

A Markov decision process is given by a set of states $S$, a set of actions $A$, and a transition function

$$
\Delta : S \times A \to \Dist(S \times \R)
$$

In words: from the state $s$ and playing action $a$, the outcome is drawn according to the distribution $\Delta(s,a)$, it yields a reward $r$ and leads to state $s'$ with probability $\Delta(s,a)(s',r)$.
Sometimes, the next-state and reward distributions are given independently, and actually in many cases the reward is deterministic and not stochastic.

We also specify an initial state $s_0$.

The objective of Markov decision processes is to model decision-making for a controller in a stochastic environment with immediate rewards.
Indeed, at every step a reward is observed (although in many scenarios we will consider the reward will be $0$ at each step except for the terminal step).

The distinctive properties of Markov decision processes is the **Markov property**: the distribution for the next state and the reward depend only on the **current** state and action, and not on the whole history. One commonly say "given the present, the future does not depend on the past", although I'm not sure this formulation helps.

The decisions are represented by strategies, which are functions $\sigma : S \to \Dist(A)$.
Note that there is a more general notion of strategies, as indeed it could be the case that the choice of the action depends on the whole history rather than only on the current state.
It turns out that the simple strategies we consider here (**positional strategies**) will always be enough for our purposes (we will prove this in due course).

An MDP $\M$ together with a strategy $\sigma$ induce a Markov chain, and in particular a probability measure $\P_\sigma$, or $\P_{\sigma,s_0}$ when specifying the initial state $s_0$.
(We will not go into the formal definitions of the Markov chain and the measure here.)
We let $S_i$ denote the random variable denoting the state reached after $i$ steps, and $R_i$ denote the random variable denoting the reward obtained at the $i$-th step. 

Given a strategy $\sigma$ we write $\val_\sigma$ for the value function defined by

$$
\val_\sigma(s) = \E_{\sigma,s} [\sum_i R_i],
$$

meaning the total expected reward obtained while playing $\sigma$ and starting from $s$.
The sum is a priori infinite, hence may not exist in general. 
There are two different solutions to that: 
* we fix some horizon and only look at paths of length the fixed horizon,
* we introduce a discount factor $\gamma \in (0,1)$ we consider the total expected discounted reward:

$$
\E_\sigma [\sum_i \gamma^i R_i]
$$

Since $$\gamma$$ is less than $$1$$, the rewards collected at the beginning are more important than the rewards in a more distant future.

We remark that the value function for $\sigma$ satisfies some recursive equation:

$$
\val_\sigma(s) = \sum_{a \in A} \sum_{s' \in S,r \in \R} \Delta(s,\sigma(s)(a))(s',r) (r + \gamma \val_\sigma(s'))
$$

The (algorithmic) **goal in reinforcement learning** is to find a strategy maximising the total expected reward:
$$
\val_*(s_0) = \sup_{\sigma \text{ strategy}} \val_\sigma(s_0)
$$

#### The two main tasks: evaluation and improvement

A value function is any function $v : S \to \R$.

Many algorithms will be based on the solutions for two tasks:
* **evaluation**: given a strategy $\sigma$, compute (or approximate) $v_\sigma$
* **improvement**: given a value function $v$, construct a strategy $\sigma$

The algorithm is then: starting from a strategy $\sigma$, apply alternatively evaluation and improvement to construct better and better strategies.

What seems to be a minor technical detail has unexpected consequences.
It is very tempting to solve the improvement task as follows: 
given a value function $v$ we define 

$$
\sigma(s) = \text{argmax}_{a \in A} \sum_{s',r} \Delta(s,a)(s',r) (r + \gamma v(s'))
$$

The problem is that this requires knowing the set of actions and the $\Delta$ function.
In some (most) cases, this knowledge will not be available.

This explains the introduction of the $Q$-values.
A $Q$-value is a function $q : S \times A \to \R$.
Given a strategy $\sigma$ we write $q_\sigma$ for the $Q$-function defined by

$$
q_\sigma(s,a) = \E_{\sigma,s,a} [\sum_i R_i],
$$

meaning the total expected reward obtained while playing $\sigma$ and starting from $s$ with action $a$.

Now the improvement task can be easily solved as follows:

$$
\sigma(s) = \text{argmax}_{a \in A} q(s,a)
$$

Of course there is no free lunch and dealing with $Q$-values will be (slightly) more complicated for the evaluation task.

#### Part 1: Dynamic algorithms 

The first setting we consider is by making two (unrealistic) assumptions:
* the MDP has a finite set of states and actions, which can be both tabularised
* the transition function is known

The first item means that in particular we can write programs including *for loops* on the set of states and actions,
in other words sweep over all possible states and actions.
Why making these assumptions if they are not realistic? 
Because the algorithms we will construct in this setting will be useful for the more general setting.

The two (families of) algorithms we will study are:
* **value iteration**
* **policy iteration** (sometimes called **strategy iteration** or **strategy improvement**)

Interestingly, these algorithms all fall into a larger class of algorithms, which simply alternates between solving the two tasks discussed above.

#### Part 2: Monte Carlo approach

A Monte Carlo approach is one where information is gathered through random sampling.
The goal is to remove the two assumptions above.

Removing the first assumption means that we do not store in memory the whole set of states, either because it is too large, 
or just because we do not have information about all states. 
Indeed we will focus on what we have seen so far during the simulations.

Removing the second assumption means that we no longer *know* the transition function. What does it mean?
We only assume that we have access to this black-box: given a state $s$ and an action $a$, sample the distribution $\Delta(s,a)$, and output the resulting pair $(s',r)$ of a state and a reward.
Note that by using this black-box a lot of times, we can recover (an approximation of) the transition function.
Of course, we do not want to do this systematically: for instance for most pairs $(s,a)$ we do not need a good approximation of the distribution $\Delta(s,a)$.

We will see how much can be salvaged of the dynamic algorithms using a Monte Carlo approach.

The key limitation in this approach is a new third assumption: episodes have bounded length.
In other words, in every simulation of the MDP we eventually end up in a terminal state.
The reason for this assumption is that the Monte Carlo approach will average over all episodes to update the values.

#### Part 3: Temporal difference

The temporal difference algorithm is another simulation-based algorithm very related to the Monte Carlo approach,
which lifts the assumption that episodes are finite.

The key difference is this:
* the Monte Carlo approach uses full episodes for updating the values
* the temporal difference updates the values in a step-by-step fashion

#### Part 4: Off-policy learning

The two tasks above (evaluation and improvement) suggest that we explore our MDP by constructing better and better strategies.
Although this is a good idea in general, it may be interesting to separate the two tasks, which describes the family of **off-policy learning**:
* one strategy is used for **exploration**, i.e. for finding information on the MDP
* another strategy is used for **exploitation**, i.e. for achieving the best possible expected total payoff

#### Part 5: Deep reinforcement learning

The last assumption to remove is that the set of states is discrete. 
Even if this is true (for instance, in Go, there are just a finite number of configurations), it may be interesting to represent states in a more continuous way.
One reason for that is to construct strategies playing appropriately from states they have never seen, which is bound to happen if the set of states is too large.

We will see how neural networks can be used to represent value functions and how their abilities to generalise beyond training data is crucial for constructing optimal strategies.


