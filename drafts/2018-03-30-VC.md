---
layout:     post
title:      Rademacher complexity, growth function, and VC dimension 
date:       2018-03-15 9:00:00
author:     Nathana&euml;l Fijalkow
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      enc: "{\\text{enc}}",
      deltasucc: "{\\delta_{\\text{succ}}}",
      last: "{\\text{last}}",
    }
  }
});
</script>

<p class="intro"><span class="dropcap">W</span>e define the Rademacher complexity, the growth function, and the VC dimension.</p>

<p>
Let $X$ be the set of inputs. The general learning question we consider is the following: 
we want to learn a target function $f : X \to \left\{0,1\right\}$ through a number of samples $(x_i,f(x_i))_{i \in [1,m]}$.
The function $f$ is unknown, but we assume some background knowledge in the form of a set of hypotheses $H$, which are functions $h : X \to \left\{0,1\right\}$.
Implicitely, we assume that $f \in H$; this assumption can be lifted to obtain more general results, which we will not do here.
</p>

<p>
We would like to quantify *how hard* it is to learn a function from $H$.
More precisely, we are interested in the PAC-learnability of $H$; see below for a definition.
There are two approaches:
* a combinatorial approach, called the VC dimension,
* a probabilistic approach, called the Rademacher complexity.
</p>

#### PAC learning

We let $D$ denote distributions over the inputs. The distribution $D$ is unknown, in particular we do not want the learning algorithm to depend on it.

The first notion we need is the loss function: how good is a hypothesis $h$ if the target function is $f$?
We let $L_D(h,f)$ denote the loss. Here we consider the simplest loss function, which is
$$P_{x \in D}(h(x) \neq f(x)).$$

> **Definition:**
We say that $H$ is PAC-learnable if
* for all $\varepsilon > 0$ (precision),
* for all $\delta > 0$ (confidence),
* there exists an algorithm $A$,
* there exists $m$ a number of samples,
* for all objective function $f \in H$,
* for all distributions $D$ over $X$,

we let $h$ denote the hypothesis picked by the algorithm $A$ when receiving the samples $(x_i,f(x_i))_{i \in [1,n]}$,
then
$$ P_{x \sim D^m} ( L(h,f) \le \varepsilon ) \ge 1 - \delta. $$

#### VC dimension

We say that a subset $Y$ of $X$ is *shattered* by $H$ if by considering the restrictions of $H$ to $Y$ we obtain all functions $Y \to \left\{0,1\right\}$.
In other words, if 
$$ \text{Card} \left\{ h_{\mid Y} : Y \to \left\{0,1\right\} \mid h \in H \right\} = 2^Y $$

Intuitively, $Y$ being shattered by $H$ means that to learn how the function $f$ behaves on $Y$ one needs the values of $f$ on each element of $Y$.
Hence the if $H$ shatters large sets, it is difficult to learn a function from $H$.

> **Definition:** The VC dimension of $H$ is the size of the largest set $Y$ which is shattered by $H$.

We unravel the definition: the VC dimension of $H$ is the size of the *largest* set $Y$ for which *all* possible functions are realised.

The beauty here is that this is a purely combinatorial definition, which abstracts away probabilities and in particular the probabilistic distribution on the inputs.

#### Rademacher complexity

The Rademacher complexity qantifies 



